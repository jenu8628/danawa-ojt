

## Machine Learning (ML)

> 선형 회귀(Linear Regression)
>
> 가설 (Hypothesis)
>
> 비용함수(Cost function)

### 1. Regression

> "Regression toward the mean"
>
> 전체의 평균으로 되돌아간다.

### 2. Linear Regression(선형 회귀)

> 데이터를 가장 잘 표현하는 직선의 방정식을 찾는 것
>
> 방법은 오차의 제곱의 합의 평균이 제일 작은 것을 찾는것이다.

![image-20220107150312176](머신러닝.assets/image-20220107150312176.png)

- 파란점이 데이터를 표현한다면 그 데이터를 가장 잘 대변하는 직선의 방정식을 찾는 것
- 직선의 방정식의 기울기(a)와 y절편(b) 값을 찾아가는 것

- 예시

![image-20220107150444291](머신러닝.assets/image-20220107150444291.png)

- 가설

![image-20220107150550027](머신러닝.assets/image-20220107150550027.png)



- 비용

![image-20220107150606642](머신러닝.assets/image-20220107150606642.png)

- 비용이 가작 적은 것을 어떻게 알 수 있을 까?
- (가설 - 실제값) 으로 구하자!
- 그런데 음수값과 양수값으로 나누어진다.
  - 제곱으로 해결!!

![image-20220107150743408](머신러닝.assets/image-20220107150743408.png)

- 오차 제곱의 평균으로 정의!

![image-20220107150754981](머신러닝.assets/image-20220107150754981.png)

- 결국 우리의 목표는 Cost를 최대한 작게하는 것!

  ![image-20220107150821659](머신러닝.assets/image-20220107150821659.png)



### 3. Simple Linear Regression(단순 선형 회귀)

```python
import tensorflow as tf

# Linear : 선형, regression : 회귀
# Hypothesis : 가설
# Cost function : 비용 함수

# H(x) Wx + b

x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

# constant : 상수 선언
# Variable : 변수


w = tf.Variable(2.9)
b = tf.Variable(0.5)

hypothesis = w * x_data + b

# cost(W, b) = ((H(x**i) - (y**2))**2)의 합 * 1/m
# 가설에서 실제값을 뺀 것의 제곱들의 평균을 나타냄
# hypothesis - y_data : 에러 값
cost = tf.reduce_mean(tf.square(hypothesis - y_data))

# tf.reduce_mean()에 대해서 알아보자.
# reduce : 줄어든다의 의미로 차원을 하나 줄인다라고 생각
# 1차원
v = [1., 2., 3., 4.]
# 0차원
tf.reduce_mean(v)   # 2.5

# tf.square() : 넘겨받은 값을 제곱함
tf.square(3)    # 9


# Gradient(경사) descent(하강) : 경사 하강 알고리즘
# Cost가 최소가 되도록 하는 미니마이즈 알고리즘 중 가장유명함
# 경사를 하강하면서 minimize cost(W,b)의 W, b를 찾는 알고리즘이다.
learning_rate = 0.01
for i in range(100):
    # with ... as 파일 객체:
    # 파일이나 함수를 열고 해당 구문이 끝나면 자동으로 닫는다.
    # close같은 것을 빼먹는 경우를 위해 만들어졌음
    # GradientTape()은 보통 with 구문과 같이 쓰임
    # with 블럭 안에 있는 변수들의 변화, 정보를 tape에 기록한다.
    # tape의 gradient 메서드를 호출해서 이후에 경사도값 즉, 미분값(기울기)을 구함
    with tf.GradientTape() as tape:
        hypothesis = w * x_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    # gradient는 변수들에 대한 개별 미분값(기울기)을 구해서 tuple로 반환한다.
    # w_grad : cost함수에서 w의 미분값(기울기), b_grad : cost함수에서 b의 미분값(기울기)
    w_grad, b_grad = tape.gradient(cost, [w, b])
    # w, b의 값을 업데이트
    # assign_sub는 뺀값을 다시 그 값에 할당해줌
    # ex) A.assign_sub(B)
    # A = A-B, A -= B
    # learning_rate는 w_grad(gradient)값을 얼마나 반영할 지 정한다.
    # 작은 값이 쓰임
    # 기울기를 구했을 때 그 기울기를 얼마만큼 반영할 것인가이다.
    w.assign_sub(learning_rate * w_grad)
    b.assign_sub(learning_rate * b_grad)
    if i % 10 == 0:
        print('{:5}|{:10.4f}|{:10.4}|{:10.6f}'.format(i, w.numpy(), b.numpy(), cost))

# 우리가 만든 가설이 얼마나 맞는지 새로운 데이터로 해보자.
# 거의 근사한 값이 나온다는 것을 알 수 있다.
print(w*5 + b)  # tf.Tensor(5.0066934, shape=(), dtype=float32)
print(w*2.5 + b)    # tf.Tensor(2.4946523, shape=(), dtype=float32)
```



![image-20220107162317978](머신러닝.assets/image-20220107162317978.png)