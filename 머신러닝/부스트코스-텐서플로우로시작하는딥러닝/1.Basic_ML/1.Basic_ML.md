

# Machine Learning (ML)

> 1. Linear Regression(선형 회귀)
> 2. Simple Linear Regression(단순 선형 회귀)
> 3. Multi-variable Linear Regression(다중 선형 회귀)
>
> 핵심 키워드
>
> - 선형 회귀(Linear Regression)
>
> - 가설 (Hypothesis)
>
> - 비용함수(Cost function)
>
> - 경사하강알고리즘(Gradient Descent algorithm)

### 기본 개념

#### 지도학습(Supervised learning)

- 이메일 스팸 필터
- 시험 성적 예측

- Regression

  - "Regression toward the mean"

  - 전체의 평균으로 되돌아간다.

  - 선형 회귀
    - 공부한 시간에 따라 성적을 예측한다.
    - 단순 선형 회귀
    - 다중 선형 회귀

- Classification(분류)

  - 이진 분류(vinary classification)
    - 점수에 따라 pass or non-pass 중 무엇이냐

  - 다중 클래스 분류
    - 점수에 따라 A, B, C, D, E 중 무엇이냐



## 1. Linear Regression(선형 회귀)

> 데이터를 가장 잘 표현하는 직선의 방정식을 찾는 것
>
> 방법은 오차의 제곱의 합의 평균이 제일 작은 것을 찾는것이다.
>
> 핵심 키워드
>
> - 가설 (Hypothesis)
> - 비용 함수(Cost function) : 평균 제곱 오차(MSE)
> - 옵티마이저(Optimizer, 최적화알고리즘) : 경사하강법(Gradient Descent)
> - 학습률(learning rate)
> - 행렬(Matrix)

![image-20220107150312176](1.Basic_ML.assets/image-20220107150312176.png)

- 파란점이 데이터를 표현한다면 그 데이터를 가장 잘 대변하는 직선의 방정식을 찾는 것
- 직선의 방정식의 기울기(a)와 y절편(b) 값을 찾아가는 것

- 예시

![image-20220107150444291](1.Basic_ML.assets/image-20220107150444291.png)

- 가설

![image-20220107150550027](1.Basic_ML.assets/image-20220107150550027.png)



- 비용

![image-20220107150606642](1.Basic_ML.assets/image-20220107150606642.png)

- 비용이 가작 적은 것을 어떻게 알 수 있을 까?
- (가설 - 실제값) 으로 구하자!
- 그런데 음수값과 양수값으로 나누어진다.
  - 제곱으로 해결!!

![image-20220107150743408](1.Basic_ML.assets/image-20220107150743408.png)

- 오차 제곱의 평균으로 정의!

![image-20220107150754981](1.Basic_ML.assets/image-20220107150754981.png)

- 결국 우리의 목표는 Cost를 최대한 작게하는 것!

  ![image-20220107150821659](1.Basic_ML.assets/image-20220107150821659.png)



## 2. Simple Linear Regression(단순 선형 회귀) 맛보기

```python
import tensorflow as tf

# Linear : 선형, regression : 회귀
# Hypothesis : 가설
# Cost function : 비용 함수

# H(x) Wx + b

x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

# constant : 상수 선언
# Variable : 변수


w = tf.Variable(2.9)
b = tf.Variable(0.5)

hypothesis = w * x_data + b

# cost(W, b) = ((H(x**i) - (y**2))**2)의 합 * 1/m
# 가설에서 실제값을 뺀 것의 제곱들의 평균을 나타냄
# hypothesis - y_data : 에러 값
cost = tf.reduce_mean(tf.square(hypothesis - y_data))

# tf.reduce_mean()에 대해서 알아보자.
# reduce : 줄어든다의 의미로 차원을 하나 줄인다라고 생각
# 1차원
v = [1., 2., 3., 4.]
# 0차원
tf.reduce_mean(v)   # 2.5

# tf.square() : 넘겨받은 값을 제곱함
tf.square(3)    # 9


# Gradient(경사) descent(하강) : 경사 하강 알고리즘
# Cost가 최소가 되도록 하는 미니마이즈 알고리즘 중 가장유명함
# 경사를 하강하면서 minimize cost(W,b)의 W, b를 찾는 알고리즘이다.
learning_rate = 0.01
for i in range(100):
    # with ... as 파일 객체:
    # 파일이나 함수를 열고 해당 구문이 끝나면 자동으로 닫는다.
    # close같은 것을 빼먹는 경우를 위해 만들어졌음
    # GradientTape()은 보통 with 구문과 같이 쓰임
    # with 블럭 안에 있는 변수들의 변화, 정보를 tape에 기록한다.
    # tape의 gradient 메서드를 호출해서 이후에 경사도값 즉, 미분값(기울기)을 구함
    with tf.GradientTape() as tape:
        hypothesis = w * x_data + b
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))

    # gradient는 변수들에 대한 개별 미분값(기울기)을 구해서 tuple로 반환한다.
    # w_grad : cost함수에서 w의 미분값(기울기), b_grad : cost함수에서 b의 미분값(기울기)
    w_grad, b_grad = tape.gradient(cost, [w, b])
    # w, b의 값을 업데이트
    # assign_sub는 뺀값을 다시 그 값에 할당해줌
    # ex) A.assign_sub(B)
    # A = A-B, A -= B
    # learning_rate는 w_grad(gradient)값을 얼마나 반영할 지 정한다.
    # 작은 값이 쓰임
    # 기울기를 구했을 때 그 기울기를 얼마만큼 반영할 것인가이다.
    w.assign_sub(learning_rate * w_grad)
    b.assign_sub(learning_rate * b_grad)
    if i % 10 == 0:
        print('{:5}|{:10.4f}|{:10.4}|{:10.6f}'.format(i, w.numpy(), b.numpy(), cost))

# 우리가 만든 가설이 얼마나 맞는지 새로운 데이터로 해보자.
# 거의 근사한 값이 나온다는 것을 알 수 있다.
print(w*5 + b)  # tf.Tensor(5.0066934, shape=(), dtype=float32)
print(w*2.5 + b)    # tf.Tensor(2.4946523, shape=(), dtype=float32)
```



![image-20220107162317978](1.Basic_ML.assets/image-20220107162317978.png)



## 3. Linear Regression과 어떻게 Cost를 줄일 수 있을까?

> 경사하강법(Gradient Descent)과 볼록함수(Convex function)

### 3-1) 경사 하강법(Gradient Descent algorithm)

- Cost를 최소화하는 방법
- 변수가 여러개일 때도 사용 가능
- 최초의 추정 또는 랜덤값을 통해 w, b값을 지정
- W나 b값을 cost(W, b)값이 줄어들 수 있도록 w와 b값을 기울기 값을 통해서 지속적으로 바꾼다.
- 최소값에 도달했다 라고 판단될 때까지 반복한다.
- 미분계산은 tensor에서 지원하므로 미분하는 코드는 작성할일 없음.

![image-20220107164429438](1.Basic_ML.assets/image-20220107164429438.png)

- m으로 나눠도되고 2m으로 나눠도 되고, 3m... 다 가능하다.
- cost특성에는 큰 영향을 주지 않기 때문에
- 미분할 때 뒤의 지수가 앞으로 나오게 되므로 약분을 통해 식이 좀 더 간략해짐
- 큰의미 둘 필요 없음 

![image-20220107164519727](1.Basic_ML.assets/image-20220107164519727.png)

- weight값을 지속적으로 업데이트
- 알파값은 우리가 구한 기울기를 얼마나 반영할 것인지 정해줌
- 0.001이나 0.0001같이 작은값을 주로 이용함
- 라운드는 편미분기호임.
- W에 대해서만 미분한다 라는 표시

![image-20220107164825449](1.Basic_ML.assets/image-20220107164825449.png)

![image-20220107165337525](1.Basic_ML.assets/image-20220107165337525.png)

### 3-2) 볼록 함수(Convex function)

- 로컬 미니멈 : 기울기가 0인 지점, 주변을 둘러봤을 때 주변에 비해서 가장 낮은 지점이라고 생각하면됨.
- 시작점에 따라 최저점이 아닌 로컬미니멈에 도달하면 나올 수 없기 때문에
- 경사하강법은 로컬미니멈이 많은경우 사용 불가

![image-20220107165550566](1.Basic_ML.assets/image-20220107165550566.png)

- 볼록 함수란 로컬미니멈과 글로벌미니멈이 일치하는 경우를 말한다.
- 즉, 로컬미니멈이 여러개 존재하지 않는 경우를 말한다.
- 이 경우 어디서 시작하든 항상 최저점에 도달하리란 것을 보장하기 때문에 경사하강법을 쓸수있다.

![image-20220107170041705](1.Basic_ML.assets/image-20220107170041705.png)

- 미분하는 식을 보여주거나 미분그래프를 그려주는 사이트가 있다는 걸 알아두자.

![image-20220107172449844](1.Basic_ML.assets/image-20220107172449844.png)



## 4. 선형회귀와 비용최소화를 Tensor Flow로 구현하기

- 비용함수(cost function)를 순수 python으로 구현해보기

```python
# 비용 함수
def cost_func(w, x, y):
    c = 0
    for i in range(len(x)):
        c += (w*x[i] - y[i]) ** 2
    return c/len(x)

# feed_w : -3~5까지 15개의 구간값
for feed_w in np.linspace(-3, 5, num=15):
    curr_cost = cost_func(feed_w, x, y)
    print("{:6.3f} | {:10.5f}".format(feed_w, curr_cost))
    
# -3.000 |   74.66667
# -2.429 |   54.85714
# -1.857 |   38.09524
# -1.286 |   24.38095
# -0.714 |   13.71429
# -0.143 |    6.09524
#  0.429 |    1.52381
#  1.000 |    0.00000
#  1.571 |    1.52381
#  2.143 |    6.09524
#  2.714 |   13.71429
#  3.286 |   24.38095
#  3.857 |   38.09524
#  4.429 |   54.85714
#  5.000 |   74.66667
```

- cost function의 그래프

![image-20220107173242434](1.Basic_ML.assets/image-20220107173242434.png)

- 비용함수(cost function)를 Tensor Flow로 구현

```python
x = np.array([1, 2, 3])
y = np.array([1, 2, 3])
def cost_func(w, x, y):
    hypothesis = x * w
    return tf.reduce_mean(tf.square(hypothesis - y))
w_values = np.linspace(-3, 5, 15)
cost_values= []
for feed_w in w_values:
    curr_cost = cost_func(feed_w, x, y)
    cost_values.append(curr_cost)
    print("{:6.3f} | {:10.5f}".format(feed_w, curr_cost))

# -3.000 | 74.66667
# -2.429 | 54.85714
# -1.857 | 38.09524
# -1.286 | 24.38095
# -0.714 | 13.71429
# -0.143 | 6.09524
# 0.429 | 1.52381
# 1.000 | 0.00000
# 1.571 | 1.52381
# 2.143 | 6.09524
# 2.714 | 13.71429
# 3.286 | 24.38095
# 3.857 | 38.09524
# 4.429 | 54.85714
# 5.000 | 74.66667
```

- Gradient descent(경사하강법)을 Tensor Flow로 구현

![image-20220107174025159](1.Basic_ML.assets/image-20220107174025159.png)

```python
alpha = 0.01
# reduce_mean : 평균, multiply : 곱하기
# ((w*x -y)x)/m
gradient = tf.reduce_mean(tf.multiply(tf.multiply(w, x) - y, x))
# descent : 새로운 w 값
# 기울기에 alpha값을 곱하고 w에서 뺀다.
descent= w - tf.multiply(alpha, gradient)
w.assign(descent)
```



- 전체코드

```python
import tensorflow as tf
tf.random.set_seed(0)

x = [1., 2., 3., 4.]
y = [1., 3., 5., 7.]
w = tf.Variable(tf.random.normal([1], -100., 100.))
for step in range(300):
    hypothesis = w*x
    cost = tf.reduce_mean(tf.square(hypothesis - y))
    alpha = 0.01
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(w, x) - y, x))
    descent= w - tf.multiply(alpha, gradient)
    w.assign(descent)

    if step % 10 == 0:
        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), w.numpy()[0]))
        
# 케라스 내장함수를 활용한 구현
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

# 케라스를 이용한 표현
x = [1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [11, 22, 33, 44, 53, 66, 77, 87, 95]

model = Sequential()

model.add(Dense(1, input_dim=1, activation='linear'))

sgd = optimizers.SGD(lr=0.01)

model.compile(optimizer=sgd, loss='mse', metrics=['mse'])

model.fit(x, y, epochs=300)
print(model.predict([8.5]))
```



## 5. Multi-variable Linear Regression(다중 선형 회귀, 다변수 선형 회귀)

> 선형회귀에서 변수가 여러개인 것이라 생각하자
>
> 핵심키워드
>
> - 가설
> - 비용함수
> - 경사하강법
> - 행렬(Matrix)

### 5-1) 다중 선형 회귀의 기초 개념과 행렬식 표현

![image-20220114093912257](1.Basic_ML.assets/image-20220114093912257.png)

![image-20220114104709709](1.Basic_ML.assets/image-20220114104709709.png)

- 변수가 여러개일 때, 가중치도 여러개가 된다. 우리는 이것을 수식으로 표현하기 어렵기 때문에 **행렬(matrix)**의 곱을 사용한다

![image-20220114104814616](1.Basic_ML.assets/image-20220114104814616.png)

![image-20220114104832980](1.Basic_ML.assets/image-20220114104832980.png)

- 가설 함수를 행렬로 표현한다면

![image-20220114110408047](1.Basic_ML.assets/image-20220114110408047.png)

- 다음 표를 우리의 가설함수와 데이터를 행렬로 표현한다면 밑에와 같이 간편하게 표현할 수 있다.

![image-20220114110532988](1.Basic_ML.assets/image-20220114110532988.png)

- 행렬의 곱으로 연산이 되기 때문에 XW로 사용을 합니다.
- 행렬의 곱연산을 하기 위해서는 앞의 행렬의 열과 뒤의 행렬의 행의 갯수가 같아야 한다. 
- 즉, M X N 행렬과 연산을 하기 위해선 N X K의 행렬이 필요!
- 둘의 곱 연산에 대한 행렬읜 M X K 행렬이 됨.
- 우리의 가설함수에서 변수의 갯수가 n개, 데이터 셋의 갯수 m개라면,
- 가중치의 갯수도 n개가 될 것입니다.
- 따라서 연산을 하기 위해서는
- X 의 행렬은 M X N 행렬이 될것이고 W의 행렬은 N X K 행렬이 될 것이므로
- XW의 행렬 곱연산을 해야합니다.
- 쎄타와 w는 같은 내용입니다.

![image-20220114111429014](1.Basic_ML.assets/image-20220114111429014.png)



### 5-2) 다중 선형 회귀를 TensorFlow로 구현

- 테스트 데이터를 코드로 표현하기

![image-20220114162504528](1.Basic_ML.assets/image-20220114162504528.png)

- 식을 그대로 코드로 표현하기

```python
import tensorflow as tf

# 데이터
x1 = [73., 93., 89., 96., 73.]
x2 = [80., 88., 91., 98., 66.]
x3 = [75., 93., 90., 100., 70.]
y = [152., 185., 180., 196., 142.]

# 가중치
w1 = tf.Variable(tf.random.normal([1]))
w2 = tf.Variable(tf.random.normal([1]))
w3 = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))


learning_rate = 0.000001

for i in range(1000+1):
    with tf.GradientTape() as tape:
        hypothesis = (w1 * x1) + (w2 * x2) + (w3 * x3) + b
        cost = tf.reduce_mean(tf.square(hypothesis - y))
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])

    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
        print("{:5} | {:12.4f}".format(i, cost.numpy()))
```

- 행렬을 사용한 코드 구현

```python
import tensorflow as tf
import numpy as np

data = np.array([[73., 80., 75., 152.],
                 [93., 88., 93., 185.],
                 [89., 91., 90., 180.],
                 [96., 98., 100., 196.],
                 [73., 66., 70., 142.]], dtype=np.float32)
# 마지막행을 제외한 나머지
x = data[:, :-1]
# 마지막행
y = data[:, [-1]]

w = tf.Variable(tf.random.normal([3, 1]))
b = tf.Variable(tf.random.normal([1]))
def predict(x):
    return tf.matmul(x, w) + b


learning_rate = 0.00001
n_epochs = 2000
for i in range(n_epochs+1):
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean(tf.square(predict(x) - y))
    w_grad, b_grad = tape.gradient(cost, [w, b])
    w.assign_sub(learning_rate * w_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
        print("{:5} | {:12.4f}".format(i, cost.numpy()))
```



- Matrix를 사용하는 것의 이점 : 코드가 깔끔해짐

![image-20220114170355457](1.Basic_ML.assets/image-20220114170355457.png)



## 6. Logistic Regression/Classification의 소개

> 핵심 키워드
>
> - 로지스틱 회귀/ 분류(Logistic Regression/ Classification)
> - 가설(Hypothesis)
> - 시그모이드/ 로지스틱(Sigmoid, Logistic)

![image-20220114174603312](1.Basic_ML.assets/image-20220114174603312.png)

### 6-1) 로지스틱 회귀란?

#### 로지스틱 회귀로 나타낼 수 있는 데이터는 Classification(분류)가 가능해야 한다.

- Binary Classification(이진 분류)

  - 시험: Pass of Fail

  - Spam : Not Spam or Spam(스팸 메일이다 아니다)

  - Face : Real or Fake(진짜다 아니다)

  - Tumor : Not Malignant

- Multi Classfication

  - 성적 : A, B, C, D, E

![image-20220114171448143](1.Basic_ML.assets/image-20220114171448143.png)

- 로지스틱을 나타내기 위한 데이터가 구분이 된다. (One Hot)
  - One-hot Encoding : 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식
- 선형회귀를 나타내기 위한 데이터는 연속된 값이다.

- 공부 시간에 따른 시험 Pass/Fail을 선형회귀로 나타낸다면 해결 할 수 가 없다.

![image-20220114172117310](1.Basic_ML.assets/image-20220114172117310.png)

- 이러한 분류(Classfication)의 경우 우리가 원하는 그래프는 이러한 모양이어야 할 것이다.

![image-20220114172159419](1.Basic_ML.assets/image-20220114172159419.png)

### 6-2) 시그모이드(Sigmoid Function)

>  선형 함수로는 분류를 나타내기 어렵다
>
> 따라서 우리가 세울 가설(hypothesis) 함수는 시그모이드 함수(Sigmoid function)를 따라간다.

![image-20220114172409802](1.Basic_ML.assets/image-20220114172409802.png)

- 시그모이드의 기본 함수는 0.5를 기준으로 0과 1을 구분할 수 있다.

![image-20220114172930027](1.Basic_ML.assets/image-20220114172930027.png)

- Decision Boundary : 경계면
- 2번째 그림과 세번째 그림의 경우에는 저러한 선과 원이 경계면이 될 수 있을 것이다.



### 6-3) Cost Function(비용 함수)

![image-20220114174840688](1.Basic_ML.assets/image-20220114174840688.png)

![image-20220114174930862](1.Basic_ML.assets/image-20220114174930862.png)
