# 참고 문헌

**한국어 임베딩 - 이기창 지음**

**github - https://github.com/ratsgo/embedding**

정리 - https://ratsgo.github.io/embedding/

# 목차

- **1장 서론**
  - **1.1 임베딩이란**
  - **1.2 임베딩의 역할**
  - **1.3 임베딩 기법의 역사와 종류**
  - **1.5 이 책이 다루는 데이터와 주요 용어**

- **2장 벡터가 어떻게 의미를 가지게 되는가**
  - **2.2 어떤 단어가 많이 쓰였는가(백오프워즈 가정)**
  - **2.3 단어가 어떤 순서로 쓰였는가(언어 모델)**
  - **2.4 어떤 단어가 같이 쓰였는가(분포 가정)**

- **3장 한국어 전처리**
  - **3.1 데이터 확보**
  - **3.2 지도 학습 기반 형태소 분석**
  - **3.3 비지도 학습 기반 형태소 분석**



# 1장 서론

## 1.1 임베딩이란

- 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체





## 1.2 임베딩의 역할

- 단어/문장 간 관련도 계산
- 의미적/문법적 정보 함축
- 전이학습

### 1.2.1 단어/문장 간 관련도 계산

단어를 벡터로 임베딩하는 순간 단어 벡터들 사이의 유사도를 계산하는 일이 가능함

- 대표적 : Word2Vec
- 희망 - 소망, 절망 - 체념, 자동차 - 승용차 ....

### 1.2.2 의미적/문법적 정보 함축

단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. 구체적으로는 **첫 번째 단어 벡터 - 두 번째 단어 벡터 + 세 번째 단어 벡터**를 계산해보는 것

- 아들 - 딸 + 소녀 = 소년

### 1.2.3 전이 학습

임베딩을 **다른 딥러닝 모델의** **입력값**으로 쓰는 기법

- 의미적, 문법적 정보 등이 녹아 있는 임베딩을 입력값으로 쓰는 전이 학습 모델은 수행하려는 테스크의 성능 역시 올라감





## 1.3 임베딩 기법의 역사와 종류

### 1.3.1 통계 기반에서 뉴럴 네트워크 기반으로

초기 임베딩 기법은 대부분 말뭉치의 통계량을 직접적으로 활용하는 경향이 있었음

- 대표적인 기법
  - 잠재 의미 분석 : 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 커다란 행렬에 특이값 분해 등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법
- 희소행렬 : 행렬 대부분의 요소 값이 0인 행렬
- 단어-문서 행렬은 말뭉치 전체의 어휘수와 같기 때문에 보통 행의 개수가 많고 행렬 대부분의 요소 값은 0이다.
- 희소 행렬을 다른 모델의 입력값으로 사용하게 되면 계산량도 메모리 소비량도 쓸데없이 커지는 문제점 때문에 원래 행렬의 차원을 축소해 사용한다.

최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목된다.

- 뉴럴 네트워크 기반 모델들은 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나, 문장 내 일부분에 구멍을 뚫어 놓고 해당 단어가 무엇일지 맞추는 과정에서 학습된다.

### 1.3.2 단어 수준에서 문장 수준으로

2017년 이전의 임베딩 기법들은 대개 단어 수준 모델이었다.

- NPLM, Word2Vec, GloVe, FastText, Swivel 등이 여기 속한다.
- 동음이의어를 분간하기 어렵다는 단점이 있다.

2018년 초 ELMo(Embeddings from Language Model)가 발표된 이후 문장 수준 임베딩 기법들이 주목을 받았다.

- BERT나 GPT등이 여기 속한다.
- 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과가 좋은 것으로 알려져 있다.

### 1.3.3 룰 → 엔드투엔드 → 프리트레인/파인 튜닝

**룰** : 한국어에서 명사 앞에 관형사가 올 수 있고 조사가 명사 뒤에 오는 경향이 있으므로 이러한 규칠을 모델에 알려주는 것

**엔드투엔드 모델** : 데이터를 통째로 모델에 넣고 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 유도하는 것

**프리트레인** : 대규모 말뭉치로 임베딩을 만드는 것

- 임베딩에는 말뭉치의 의미적, 문법적 맥랑이 포함돼 있다.

**파인튜닝, 전이학습** : 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트

### 1.3.4 임베딩의 종류와 성능

- 행렬 분해
- 예측
- 토픽 기반 방법

**행렬 분해 기반 방법**

- 말뭉치 정보가 들어 있는 기존 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법
- Glove, Swivel 등이 여기에 속함

**예측 기반 방법**

- 어떤 단어 주변에 특정 단어가 나타날지 예측하거나
- 이전 단어들이 주어 졌을 때 다음 단어가 무엇일지 예측하거나
- 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법
- Word2Vec, FastText, BERT, ELMo, GPT 등이 여기에 속함

**토픽 기반 방법**

- 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 기법
- **잠재 디리클레 할당**이 대표적인 기법이다.





## 1.5 이 책이 다루는 데이터와 주요 용어

- 말뭉치 : 임베딩에 사용할 모든 데이터
  - 임베딩 학습이라는 특정한 목적을 가지고 수집한 표본
  - ex ) 한국어 위키백과와 네이버 영화 리뷰 데이터를 임베딩에 쓴다고 가정하면 그 둘을 모두 합친 것
- 컬렉션 : 말뭉치에 속한 각각의 집합
  - ex) 국어 위키백과와 네이버 영화 리뷰를 말뭉치로 쓴다면 이들 각각이 컬렉션이 됨
- 문장 : 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 독립적인 형식 단위
  - 이 책이 다루는 데이터의 기본 단위
- 문서 : 생각이나 감정, 정보를 공유하는 문장 집합
- 토큰 : 이 책에서 다루는 가장 작은 단위
  - 단어
  - 형태소
  - 서브워드
- 토크나이즈 : 문장을 토큰 시퀀스로 분석하는 과정
  - 실수, 인, 초월수, 는, 모두, 무리수, 이, 다, .
- 어휘 집합 : 말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토크나이즈를 실시한 후 중복을 제거한 토큰들의 집합
- 미등록 단어 : 어휘 집합에 없는 토큰
- 시퀀스 데이터 : 순서(sequence)가 있는 데이터
  - 시퀀스 원소들은 특정 순서를 가지므로 독립적이지 않다.
  - 대표적으로 시계열 데이터(시간의 흐름에 따라 기록된 데이터)와 텍스트 데이터(텍스트도 쪼개어 보면 시간에 따라 문맥이 존재. 즉, 순서가 존재)





# 2장 벡터가 어떻게 의미를 가지게 되는가

## 2.1 자연어 계산과 이해

임베딩에 자연어 의미를 함축하는 방법 : 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것

임베딩을 만들 때 쓰는 통계 정보의 세 가지 철학

- 문장에 어떤 단어가 (많이)쓰였는가 (백오브워즈 가정)
- 단어가 어떤 순서로 쓰였는가 (언어 모델)
- 어떤 단어가 같이 쓰였는가 (분포 가정)

위의 세 가지 내용은 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호 보완적이다.





## 2.2 어떤 단어가 많이 쓰였는가

### 2.2.1 백오브워즈 가정

단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법

- 백(bag) : 중복 원소를 허용한 집합
- 문장을 단어들로 나누고 이들을 중복 집합에 넣어 임베딩으로 활용하는 것
- '저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다 '는 가정이 깔려 있다.
- **정보 검색** 분야에서 여전히 많이 쓰이고 있다.

### 2.2.2 TF-IDF

어떤 문서에든 쓰여서 해당 단어가 (많이) 나타났다 하더라도 문서의 주제를 가늠하기 어려운 경우가 있다.

'을/를', '이/가' 같은 조사는 대부분의 한국어 문서에 등장하지만 이것 만으로는 해당 문서의 주제를 추측하기 어렵다.

이러한 단점을 보완하기 위해 제안된 기법이 Term Frequency-Inverse Document Frequency이다.

- 가중치를 계산해 행렬 원소를 바꾸는 것

![image-20220408090909388](임베딩.assets/image-20220408090909388.png)

![image-20220408090918851](임베딩.assets/image-20220408090918851.png)

- TF : 특정 문서에서 특정 단어의 등장 횟수
- DF : 특정 단어가 등장한 문서의 수
  - DF가 클수록 다수 문서에 쓰이는 범용적인 단어라고 볼 수 있다.
- TF는 같은 단어라도 문서마다 다른 값을 갖고, DF는 문서가 달라지더라도 단어가 같다면 동일한 값을 지닌다.
- IDF : 전체 문서의 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값
  - N이 커질수록 그 값이 기하급수적으로 커지게 되어 로그를 취함
  - 값이 클수록 특이한 단어라는 뜻
  - 단어의 주제 예측 능력과 직결됨
  - 특정 단어가 다수의 문서에서 발견될수록 IDF값이 줄어든다(중요도가 낮다) → 다수의 문서에 쓰였다면 조사와 같이 의미 없는 단어일 확률이 높다.

### 2.2.3 Deep Averaging Network

백오브워즈 가정의 뉴럴 네트워크 버전





## 2.3 단어가 어떤 순서로 쓰였는가

> 언어 모델 : 단어 시퀀스에 확률을 부여하는 모델
>
> - 백오브워즈의 대척점으로 시퀀스 정보를 명시적으로 학습
> - 단어가 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률, 즉 P(w1, w2, ... , wn)을 반환

### 2.3.1 통계 기반 언어 모델

말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습

n-gram : n개 단어

- 경우에 따라 n-gram에 기반한 언어 모델을 의미
- 말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 뜻

'내 마음 속에 영원히 기억될 최고의' 라는 표현 다음에 명작이다 라는 단어가 나타날 확률

**조건부확률**의 정의를 활용해 **최대우도추정법**으로 유도하면

- P(명작이다 | 내, 마음, 속에, 영원히, 기억될, 최고의) = Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다) / Freq(내, 마음, 속에, 영원히, 기억될, 최고의)
  - Freq : 해당 문자열 시퀀스가 말뭉치에서 나타난 빈도
  - Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다) = 0 (즉, 단어가 등장한 적이 없다.)
  - 이런 경우 우변의 분자가 0이어서 전체 값이 0이됨.

**바이그램 근사**

- n-gram 모델을 쓰면 위의 문제를 일부 해결 가능
- 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사하는 것
- 이는 한 상태의 확률은 그 직전 상태에만 의존한다는 마코프 가정에 기반
- ex) P(명작이다 | 내, 마음, 속에, 영원히, 기억될, 최고의) ≒ P(명작이다 | 최고의) = Freq(최고의, 명작이다) / Freq(최고의)

바이그램 모델에서 '내 마음 속에 영원히 기억될 최고의 명작이다' 라는 단어 시퀀스가 나타날 확률은?

- P(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)
- ≒ P(내) * P(마음|내) * P(속에|마음) * P(영원히|속에) * P(기억될|영원히) * P(최고의|기억될) * P(명작이다|최고의)

바이그램 모델 일반화

- P(Wn | Wn-1) = Freq(Wn-1, Wn) / Freq(Wn-1)

![image-20220408090844490](임베딩.assets/image-20220408090844490.png)

하지만 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 단계에서 문제가 발생할 수 있다.

이를 위해 **백오프, 스무딩** 등의 방식이 제안됐다.

- 백오프

   

  : n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식

  - n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높기 때문
  - Freq(내 마음 속에 영원히 기억될 최고의 명작이다) ≒ αFreq(영원히 기억될 최고의 명작이다) + β
    - α, β : 실제 빈도와의 차이를 보정해주는 파라미터
    - 빈도가 1 이상인 n-gram에 대해서는 백오프하지 않고 해당 빈도를 그대로 n-gram 모델 학습에 사용

- 스무딩 : 

  등장 빈도 표에 모두 k만큼을 더하는 기법

  - 빈도가 0인 케이스를 없앨 수 있다.
  - **Add-k 스무딩**이라고 부르기도 함
  - 만약 k = 1 이라면 **라플라스 스무딩**이라고 한다.
  - 스무딩을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고 학습 데이터에 전혀 등장하지 않는 케이스들에는 작으나마 일부 확률을 부여하게 됨

### 2.3.2 뉴럴 네트워크 기반 언어 모델

ex) 발없는 말이 → 언어모델 → 천리

단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습됨

**※ 마스크 언어 모델 :** 언어 모델 기반 기법과 큰 틀에서 유사하지만 디테일에서 차이를 보이는 기법

- ex) 발 없는 말이 [MASK] 간다 → 언어 모델 → 천리
- 언어 모델 기반 기법은 단어를 순차적으로 입력 받아 다음 단어를 맞춰야 하기 때문에 태생적으로 **일방향**이다.
- But 마스크 언어 모델은 문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 **양방향** 학습이 가능
- 이로 인해 마스크 언어 모델 기반의 방법들은 기존 언어 모델 기법들 대비 임베딩 품질이 좋음





## 2.4 어떤 단어가 같이 쓰였는가

### 2.4.1 분포 가정

자연어 처리에서의 **분포 :** 특정 범위, 즉 **윈도우** 내에 동시에 등장하는 이웃 단어 또는 **문맥**의 집합

- 개별 단어의 분포는 그 단어가 문장 내에서 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라진다.

- 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는 게 **분포가정**의 전제다.

- ex) 빨래, 세탁이라는 단어의 의미를 모른다고 했을 때 여러 문장들에서 빨래, 세탁은 각각 

  타깃 단어

  로 청소, 물 등은 그 주위에 등장한 

  문맥 단어

  이다.

  - 빨래 : 청소, 요리, 물, 속옷과 같이 등장했다고 하자.
  - 세탁 : 청소, 요리, 물, 옷과 같이 등장했다고 하자.
  - 이웃한 단어들이 서로 비슷하기 때문에 빨래와 세탁은 비슷한 의미를 지닐 가능성이 높다!
  - 아울러 빨래가 청소, 요리, 물, 속옷과 같이 등장하는 경향을 보았을 때 이들끼리도 직간접적으로 관계를 지닐 가능성 역시 낮지 않다.

### 2.4.2 분포와 의미 (1) : 형태소

**형태소** : 의미를 가지는 최소 단위

- 언어 학자들이 형태소를 분석하는 방법은 조금 다르다.

- 대표적으로 

  계열 관계

  가 있다.

  - 계열 관계 : 해당 형태소 자리에 다른 형태소가 '대치'돼 쓰일 수 있는가를 따지는 것

- 이는 언어 학자들이 특정 타깃 단어 주변의 문맥 정보를 바탕으로 형태소를 확인하는 것

- 즉, 말뭉치의 분포 정보와 형태소가 밀접한 관계를 이루고 있다는 것이다.

### 2.4.3 분포와 의미 (2) : 품사

**품사** : 단어를 문법적 성질의 공통성에 따라 언어 학자들이 몇 갈래로 묶어 놓은 것

- 분류 기준 : 기능, 의미, 형식
- 예시
  - 이 샘의 **깊이**가 얼마냐?
  - 저 산의 **높이**가 얼마냐?
  - 이 샘이 **깊다**.
  - 저 산이 **높다**.
- 여기서 깊이, 높이는 문장의 주어로 싶다, 높다는 서술어로 사용됨.
- 이처럼 기능이 같은 단어 부류를 같은 품사로 묶을 수 있다.
- 품사 분류에서 가장 중요한 기준은 **기능**이다.

### 2.4.4 점별 상호 정보량(PMI)

**두 확률 변수** 사이의 상관성을 계량화하는 단위

- 두 확률 변수가 완전히 **독립**인 경우에 그 값이 0이 됨
- 독립 : A가 나타나는 것이 단어 B의 등장할 확률에 전여 영향을 주지 않고, 단어 B 등장이 단어 A에 영향을 주지 않는 경우
- 단어 A가 등장할 때 단어 B와 자주 같이 나타난다면 PMI값은 커진다

두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화 한 것

- PMI(A,B) = log(P(A,B) / (P(A) * P(B)))
- '개울가에서 속옷 빨래를 하는 남녀'
  - 개울가, 에서, 속옷, 빨래, 를, 하는, 남녀 
  - 타깃 단어 : 빨래
  - window = 2 라면
  - 문맥 단어 : 에서, 속옷, 를, 하는
- 단어 문맥 행렬을 모두 구했다고 해보자. 
  - 전체 빈도 수는 1000회, 빨래 등장 횟수 20회, 속옷 등장 횟수 15회, 빨래와 속옷이 동시 등장한 횟수 10회라고 가정하면
  - 빨래-속옷 간 PMI = PMI(빨래, 속옷) = log(P(빨래,속옷) / (P(빨래) * P(속옷))) = log((10 / 1000) / ((20 / 1000) * (15 / 1000)))

### 2.4.5 Word2Vec

분포 가정의 대표적 모델

2013 구글 연구 팀이 발표한 임베딩 기법

- CBOW 모델 : 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됨
- Skip-gram 모델 : 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습됨
- 둘 모두 특정 타깃 단어 주변의 문맥, 즉 분포 정보를 임베딩에 함축







# 3장 한국어 전처리

## 3.1 데이터 확보

- 이미 공개돼있는 말뭉치 데이터를 활용

### 3.1.1 한국어 위키백과

위키백과 : 누구나 자유롭게 수정, 편집할 수 있는 인터넷 백과사전

한국어 위키백과의 원 데이터(raw data)를 다운로드 하는 방법

- https://blog.naver.com/PostView.naver?blogId=duqrlwjddns1&logNo=222484574485

위키백과 XML 문서상의 문학 항목

- 우리가 필요로 하는 본문 텍스트만 뽑아낸다.
- 특수문자, 공백, 이메일주소, 웹 페이지 주소 등을 제거

### 3.1.2 KorQuAD

KorQuAD : 한국어 **기계 독해**를 위한 데이터셋

- 질문과 답변 쌍을 사람들이 직접 만들었다.

### 3.1.3 네이버 영화 리뷰 말뭉치

**감성 분석**이나 **문서 분류** 태스크 수행에 제격인 데이터 셋

- 레코드 하나는 문서(리뷰)에 대응

### 3.1.4 전처리 완료된 데이터 다운로드

- https://ratsgo.github.io/embedding/downloaddata.html





## 3.2 지도 학습 기반 형태소 분석

> 문장이나 단어의 경계를 컴퓨터에 알려주지 않으면 어휘 집합에 속한 단어 수가 기하급수적으로 늘어나서 연산의 비효율이 발생
>
> 특히 한국어는 조사와 어미가 발달한 **교착어**이기 때문에 섬세한 처리 필요
>
> - 가다 - 가겠다 가더라 ...
>
> 형태소 분석 기법을 사용하면 어휘 집합을 줄일 수 있다.
>
> - 가겠다 - 가, 겠, 다
> - 가더라 - 가, 더, 라
>
> 태깅 : 모델 입력과 출력 쌍을 만드는 작업
>
> - 입력 : 아버지가방에들어가신다
> - 출력 : 아버지, 가, 방, 에, 들어가, 신다

### 3.2.1 KoNLPy 사용법

은전한닢(Mecab), 꼬꼬마(Kkma), 한나눔(Hannanum), Okt, 코모란(Komoran) 등 5개 오픈소스 형태소 분석기를 파이썬 환경에서 사용할 수 있도록 인터페이스를 통일한 한국어 자연어 처리 패키지

- 품사 태그 내용 확인 : https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit

### 3.2.2 KoNLPy 내 분석기별 성능 차이 분석

- 로딩 시간 : 분석기가 사용하는 사전 로딩을 포함해 형태소 분석기 클래스를 읽어 들이는 시간
- 실행 시간 : 10만 문자의 문서를 분석하는 데 소요되는 시간

| 분석기명         | 로딩 시간 | 실행 시간 |
| ---------------- | --------- | --------- |
| Kkma(꼬꼬마)     | 5.6988    | 35.7163   |
| Komoran(코모란)  | 5.4866    | 25.6008   |
| Hannanum(한나눔) | 0.6591    | 8.8251    |
| Okt              | 1.4870    | 2.4714    |
| Mecab(은전한닢)  | 0.0007    | 0.2838    |

- Mecab이 다른 분석기 대비 속도가 빠른 편이다.
- 속도만큼 형태소 분석 품질도 중요하다.
- 자신이 가진 데이터로 시험 삼아 형태소 분석을 해보고 속도나 품질을 비교해서 고르는 것이 좋다.

### 3.2.3 Khaiii

https://tech.kakao.com/2018/12/13/khaiii/

Kakao Hangul Anlyzer Ⅲ, 카카오가 공개한 오픈소스 한국어 형태소 분석기

CNN 모델 적용

- 입력 문장을 문자 단위로 읽어 들인 뒤 **컨볼루션 필터**가 이 문자들을 슬라이딩해 가면서 정보를 추출





## 3.3 비지도 학습 기반 형태소 분석

> 데이터의 패턴을 모델 스스로 학습하게 함으로써 형태소를 분석하는 방법

### 3.3.1 soynlp 형태소 분석기

형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지

하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 **동질적인 문서 집합**에서 잘 작동한다.

- 데이터의 통계량을 확인해 만든 단어 점수 표로 작동
- 단어점수표 → 응집 확률, 브랜칭 엔트로피를 활용한다.
  - 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나고(응집 확률이 높을 때), 그 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급

```python
pip install soynlp
```

```python
from soynlp.word import WordExtractor
from soynlp.tokenizer import LTokenizer
import math

#----------------------------------------------------------------------------------------------------------------------
# soynlp 비지도 기반 형태소 분석기
corpus_fname = 'processed_ratings.txt'
model_fname = 'soyword.model'

# 예제 파일 문장 리스트로 저장
sentences = [sent.strip() for sent in open(corpus_fname, 'r', encoding='UTF8').readlines()]
# 객체 선언
word_extractor = WordExtractor(min_frequency=100,
                               min_cohesion_forward=0.05,
                               min_right_branching_entropy=0.0)
# word_extractor.train(sentences)     # 모델 학습
# word_extractor.save(model_fname)    # 모델 저장

scores = word_extractor.word_scores()
scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}

tokenizer = LTokenizer(scores=scores)
tokens = tokenizer.tokenize("애비는 종이었다")
print(tokens)
```

### 3.3.2 구글 센텐스피스

**바이트 페어 인코딩(BPE)** 기법 등을 지원

- BPE의 기본 원리 : 말뭉치에서 가장 많이 등장한 문자열을 병합해 문자열을 압축
  - ex) aaabdaaabac
  - aa → Z로 치환 : ZabdZabac
  - ab→ Y로 치환 : ZYdZYac

- BPE를 활용한 토크나이즈 매커니즘의 핵심
  - 원하는 어휘 집합 크기가 될 때까지 반복적으로 고빈도 문자열들을 병합해 어휘 집합에 추가
- 학습이 끝난 후 예측
  - 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에 어휘 집합에 있는 서브워드가 포함돼 있을 경우 해당 서브워드를 어절에서 분리
  - 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고, 또 분리
  - 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어로 취급

### 3.3.3 띄어쓰기 교정

- soynlp에서 띄어쓰기 교정 모듈 제공
- 말뭉치에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행
- soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라진다.

```python
pip install soyspacing
```

```python
from soyspacing.countbase import CountSpace

corpus_fname = 'processed_ratings.txt'

model = CountSpace()
model.train(corpus_fname)
# model.save_model('space-correct.model', json_format=False)
# model.load_model('space-correct.model', json_format=False)
model.correct("어릴때보고 지금다시봐도 재밌어요")
```







# 4장 단어 수준 임베딩