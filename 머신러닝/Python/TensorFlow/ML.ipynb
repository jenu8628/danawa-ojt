{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4131549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    2.4520|     0.376| 45.660004\n",
      "   10|    1.1036|  0.003398|  0.206336\n",
      "   20|    1.0128|  -0.02091|  0.001026\n",
      "   30|    1.0065|  -0.02184|  0.000093\n",
      "   40|    1.0059|  -0.02123|  0.000083\n",
      "   50|    1.0057|  -0.02053|  0.000077\n",
      "   60|    1.0055|  -0.01984|  0.000072\n",
      "   70|    1.0053|  -0.01918|  0.000067\n",
      "   80|    1.0051|  -0.01854|  0.000063\n",
      "   90|    1.0050|  -0.01793|  0.000059\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Linear : 선형, regression : 회귀\n",
    "# Hypothesis : 가설\n",
    "# Cost function : 비용 함수\n",
    "\n",
    "# H(x) Wx + b\n",
    "\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# constant : 상수 선언\n",
    "# Variable : 변수\n",
    "\n",
    "\n",
    "w = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "hypothesis = w * x_data + b\n",
    "\n",
    "# cost(W, b) = ((H(x**i) - (y**2))**2)의 합 * 1/m\n",
    "# 가설에서 실제값을 뺀 것의 제곱들의 평균을 나타냄\n",
    "# hypothesis - y_data : 에러 값\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "# tf.reduce_mean()에 대해서 알아보자.\n",
    "# reduce : 줄어든다의 의미로 차원을 하나 줄인다라고 생각\n",
    "# 1차원\n",
    "v = [1., 2., 3., 4.]\n",
    "# 0차원\n",
    "tf.reduce_mean(v)   # 2.5\n",
    "\n",
    "# tf.square() : 넘겨받은 값을 제곱함\n",
    "tf.square(3)    # 9\n",
    "\n",
    "\n",
    "# Gradient(경사) descent(하강) : 경사 하강 알고리즘\n",
    "# Cost가 최소가 되도록 하는 미니마이즈 알고리즘 중 가장유명함\n",
    "# 경사를 하강하면서 minimize cost(W,b)의 W, b를 찾는 알고리즘이다.\n",
    "learning_rate = 0.01\n",
    "for i in range(100):\n",
    "    # with ... as 파일 객체:\n",
    "    # 파일이나 함수를 열고 해당 구문이 끝나면 자동으로 닫는다.\n",
    "    # close같은 것을 빼먹는 경우를 위해 만들어졌음\n",
    "    # GradientTape()은 보통 with 구문과 같이 쓰임\n",
    "    # with 블럭 안에 있는 변수들의 변화, 정보를 tape에 기록한다.\n",
    "    # tape의 gradient 메서드를 호출해서 이후에 경사도값 즉, 미분값(기울기)을 구함\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    # gradient는 변수들에 대한 개별 미분값(기울기)을 구해서 tuple로 반환한다.\n",
    "    # w_grad : cost함수에서 w의 미분값(기울기), b_grad : cost함수에서 b의 미분값(기울기)\n",
    "    w_grad, b_grad = tape.gradient(cost, [w, b])\n",
    "    # w, b의 값을 업데이트\n",
    "    # assign_sub는 뺀값을 다시 그 값에 할당해줌\n",
    "    # ex) A.assign_sub(B)\n",
    "    # A = A-B, A -= B\n",
    "    # learning_rate는 w_grad(gradient)값을 얼마나 반영할 지 정한다.\n",
    "    # 작은 값이 쓰임\n",
    "    # 기울기를 구했을 때 그 기울기를 얼마만큼 반영할 것인가이다.\n",
    "    w.assign_sub(learning_rate * w_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print('{:5}|{:10.4f}|{:10.4}|{:10.6f}'.format(i, w.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19242c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ec197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
